{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Copyright (c) Microsoft Corporation. All rights reserved.*                 \n",
    "*Licensed under the MIT License.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/sh: 1: import: Permission denied\r\n"
     ]
    }
   ],
   "source": [
    "!import scrapbook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interpreting Bag of Words Text Classification models\n",
    "\n",
    "This notebook show a set by step implementation of traditional machine algorithms with a bag of words representation as an interpretable model using feature importances.\n",
    "\n",
    "###### Note:\n",
    "* This example walks through a logistic regression baseline with a simple Bag of Words encoding. However, any model that follows sklearn's classifier API should be supported natively or with minimal tweaking.\n",
    "* The interpreter supports interpretations using either coefficients associated with linear models or feature importances associated with ensemble models.\n",
    "* For larger text datasets, the classifier relies on scipy's sparse representations to keep the dataset in memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'spacy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-42ae710575b2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mworking_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'pwd'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdirname\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdirname\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mworking_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# TODO : explore methods to eliminate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mutils_interpret\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils_bow\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSpacyTokenizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mencode_labels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mutils_interpret\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils_bow\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mplot_local_imp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mget_local_importances\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mutils_interpret\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils_bow\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mget_important_words\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mplot_global_imp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/c/Users/mithigpe/interpret-text/interpret-community-text/utils_interpret/utils_bow.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# This file contains helper functions needed to run the bag of words baseline notebook\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mspacy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mhtml\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mstring\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'spacy'"
     ]
    }
   ],
   "source": [
    "#import libraries\n",
    "import sys\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy\n",
    "import string\n",
    "from scrapbook.api import glue\n",
    "working_dir = %pwd\n",
    "sys.path.append(os.path.dirname(os.path.dirname(working_dir))) # TODO : explore methods to eliminate\n",
    "from utils_interpret.utils_bow import SpacyTokenizer,encode_labels\n",
    "from utils_interpret.utils_bow import plot_local_imp, get_local_importances\n",
    "from utils_interpret.utils_bow import get_important_words, plot_global_imp\n",
    "# sklearn\n",
    "from sklearn.metrics import classification_report, accuracy_score, precision_recall_fscore_support\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.base import TransformerMixin\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "# spacy and nlp recipes\n",
    "import spacy\n",
    "from spacy.lang.en import English\n",
    "from utils_nlp.dataset.multinli import load_pandas_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "The notebook is built on features made available by [Sci-kit Learn](https://scikit-learn.org/stable/) and [spacy](https://spacy.io/) for easier compatibiltiy with popular tookits.                     \n",
    "The notebook also relies on the pip installable \"utils_nlp.dataset.multinli\" package from Microsoft's [NLP-recipes](https://github.com/microsoft/nlp-recipes/tree/master/utils_nlp) open source package for dataloading purposes.               \n",
    "\n",
    "\n",
    "### An Overview\n",
    "\n",
    "The [Bag of Words](https://en.wikipedia.org/wiki/Bag-of-words_model) allows a 1:1 mapping from the individual words to their respective frequencies in the Document-term matrix.                                \n",
    "We use spacy's medium language [model](https://spacy.io/models/en#en_core_web_md) that's trained on common crawl for text preprocessing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set configuration parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "#SPACY_LANG_MODEL = 'en_core_web_md'\n",
    "DATA_FOLDER = './temp'\n",
    "TRAIN_SIZE = 0.7\n",
    "TEST_SIZE = 0.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = load_pandas_df(DATA_FOLDER, \"train\")\n",
    "df = df[df[\"gold_label\"]==\"neutral\"]  # get unique sentences\n",
    "\n",
    "# fetch documents and labels from data frame\n",
    "X_str = df['sentence1'] # the document we want to analyze\n",
    "ylabels = df['genre'] # the labels, or answers, we want to test against"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure preprocessessing setup\n",
    "Each text document in the dataset under goes the following preprocessing steps during conversion into tokens:\n",
    "* Use spacy's English parser to parse document.\n",
    "* Convert to lower case.\n",
    "* Strip off white space surrounding each word.\n",
    "* [lemmatize](https://en.wikipedia.org/wiki/Lemmatisation) based on spacy's language model.\n",
    "* Remove stop words and punctuation to obtain tokens.\n",
    "\n",
    "## Preprocess training data and labels and cast into correct format\n",
    "* Count tokens and construct sparse term document matrix.\n",
    "* Vectorize tokens into counts.\n",
    "* Convert labels from strings into integers.\n",
    "\n",
    "###### Note: Vocabulary\n",
    "\n",
    "* *The vocabulary is compiled from the dataset. This means that any word that does not appear in the Dataset is not a part of the vocabulary. This also means that the vocabulary varies with the dataset.*           \n",
    "* *Every word that appear one or more times is considered to be part of the vocab.*\n",
    "* *However, The sklearn countvectorizer allows addition of a custom vocabulary as an input parameter.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert tokens to BOW count vector\n",
    "spacytokenizer = SpacyTokenizer(English())\n",
    "countvectorizer = CountVectorizer(tokenizer = spacytokenizer.tokenize, ngram_range=(1,1))\n",
    "X_vec = countvectorizer.fit_transform(X_str)\n",
    "\n",
    "# Encode labels as numbers\n",
    "labelencoder, ylabels = encode_labels(ylabels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure training setup\n",
    "\n",
    "* Split data into train and test using a random shuffle\n",
    "* load desired classifier. In this case, Logistic Regression.\n",
    "* Setup grid search for hyperparameter optimization and train model. Edit the hyper parameter range to search over as per your model.\n",
    "* Fit models to train set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into training and test set \n",
    "X_train, X_test, y_train, y_test = train_test_split(X_vec,ylabels, train_size = TRAIN_SIZE, \n",
    "                                                    test_size = TEST_SIZE, random_state=0)\n",
    "print(\"X_train shape =\" + str(X_train.shape))\n",
    "print(\"y_train shape =\" + str(y_train.shape))\n",
    "print(\"X_train data structure = \"+ str(type(X_train)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Note:\n",
    "* *Set the parameters using cross-validation*\n",
    "* *Below listed hyperparamters are selected by searching over a larger space.*\n",
    "* *These apply speciically to this instance of the logistic regression model and mnli dataset.*\n",
    "* *'Multinomial' setup was found to be better than 'one-vs-all' across the board*\n",
    "* default 'liblinear' solver is not supported for 'multinomial' model setup\n",
    "* *For a different model or dataset, set the range as appropriate*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuned_parameters = [{'solver': ['saga'],'multi_class': ['multinomial'], 'C': [10**4]}]\n",
    "\n",
    "model =  LogisticRegression()\n",
    "classifier_CV = GridSearchCV(model, tuned_parameters, cv=3, scoring='accuracy')\n",
    "classifier_CV.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "* Obtain best model and corresponding hyper parameters\n",
    "* Report Accuracy, F1 score, Precision and Recall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtain best classifier and hyper params\n",
    "classifier = classifier_CV.best_estimator_\n",
    "print(\"best classifier: \" + str(classifier_CV.best_params_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Report performance metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_accuracy = classifier.score(X_test, y_test, sample_weight=None)\n",
    "print(\"accuracy = \" + str(mean_accuracy*100) + \"%\")\n",
    "y_pred = classifier.predict(X_test)\n",
    "[precision, recall, fscore, support] = precision_recall_fscore_support(y_test, y_pred,average='macro')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Capture metrics for integration testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glue(\"accuracy\", mean_accuracy)\n",
    "glue(\"precision\", precision)\n",
    "glue(\"recall\", recall)\n",
    "glue(\"f1\", fscore)\n",
    "print (\"[precision, recall, fscore, support] = \" + str([precision, recall, fscore, support]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute global importances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize the most and least important words for each class over the entire dataset\n",
    "Choose which label's importances to visualize "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The class names are as follows\")\n",
    "print(labelencoder.classes_)\n",
    "label_name = \"fiction\"\n",
    "\n",
    "#Obtain the top feature ids for the selected class label.           \n",
    "#Map top features back to words.\n",
    "top_words, top_importances = get_important_words(classifier, label_name, countvectorizer, labelencoder)\n",
    "#Plot the feature importances\n",
    "plot_global_imp(top_words, top_importances, label_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute local importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Enter any document & label pair that needs to be interpreted\n",
    "document = \"I travelled to the beach. I took the train. I saw faries, dragons and elves\"\n",
    "label_name = \"travel\"\n",
    "\n",
    "#Obtain the top feature ids for the selected class label\n",
    "parsed_sentence, word_importances = get_local_importances(classifier, labelencoder, label_name,\n",
    "                                                          document, spacytokenizer, countvectorizer)\n",
    "#Visualize local feature importances as a heatmap over words in the document\n",
    "plot_local_imp(parsed_sentence, word_importances)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (interpret_cpu)",
   "language": "python",
   "name": "interpret_cpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
