{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Copyright (c) Microsoft Corporation. All rights reserved.*                 \n",
    "*Licensed under the MIT License.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interpreting Bag of Words Text Classification models\n",
    "\n",
    "This notebook show a set by step implementation of traditional machine algorithms with a bag of words representation as an interpretable model using feature importances.\n",
    "\n",
    "###### Note:\n",
    "* This example walks through a logistic regression baseline with a simple Bag of Words encoding. However, any model that follows sklearn's classifier API should be supported natively or with minimal tweaking.\n",
    "* The interpreter supports interpretations using either coefficients associated with linear models or feature importances associated with ensemble models.\n",
    "* The classifier relies on scipy's sparse representations to keep the dataset in memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from interpret_text.linear.linear_text_explainer import LinearTextExplainer\n",
    "\n",
    "# sklearn\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# nlp recipes\n",
    "from utils_nlp.dataset.multinli import load_pandas_df\n",
    "\n",
    "# for testing\n",
    "from scrapbook.api import glue\n",
    "\n",
    "working_dir = os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "The notebook is built on features made available by [Sci-kit Learn](https://scikit-learn.org/stable/) and [spacy](https://spacy.io/) for easier compatibiltiy with popular tookits.                     \n",
    "The notebook also relies on the pip installable \"utils_nlp.dataset.multinli\" package from Microsoft's [NLP-recipes](https://github.com/microsoft/nlp-recipes/tree/master/utils_nlp) open source package for dataloading purposes.               \n",
    "\n",
    "\n",
    "### An Overview\n",
    "\n",
    "The [Bag of Words](https://en.wikipedia.org/wiki/Bag-of-words_model) allows a 1:1 mapping from the individual words to their respective frequencies in the Document-term matrix.                                \n",
    "We use spacy's medium language [model](https://spacy.io/models/en#en_core_web_md) that's trained on common crawl for text preprocessing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set configuration parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_FOLDER = './temp'\n",
    "TRAIN_SIZE = 0.7\n",
    "TEST_SIZE = 0.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = load_pandas_df(DATA_FOLDER, \"train\")\n",
    "df = df[df[\"gold_label\"] == \"neutral\"]  # get unique sentences\n",
    "\n",
    "# fetch documents and labels from data frame\n",
    "X_str = df['sentence1']  # the document we want to analyze\n",
    "ylabels = df['genre'] # the labels, or answers, we want to test against"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure preprocessessing setup\n",
    "\n",
    "In the default LinearExplainer, each text document in the dataset undergoes the following preprocessing steps during conversion into tokens:\n",
    "* Use spacy's English parser to parse document.\n",
    "* Convert to lower case.\n",
    "* Strip off white space surrounding each word.\n",
    "* [lemmatize](https://en.wikipedia.org/wiki/Lemmatisation) based on spacy's language model.\n",
    "* Remove stop words and punctuation to obtain tokens.\n",
    "\n",
    "## Preprocess training data and labels and cast into correct format\n",
    "* Count tokens and construct sparse term document matrix.\n",
    "* Vectorize tokens into counts.\n",
    "* Convert labels from strings into integers.\n",
    "\n",
    "###### Note: Vocabulary\n",
    "\n",
    "* *The vocabulary is compiled from the dataset. This means that any word that does not appear in the Dataset is not a part of the vocabulary. This also means that the vocabulary varies with the dataset.*           \n",
    "* *Every word that appear one or more times is considered to be part of the vocab.*\n",
    "* *However, The sklearn countvectorizer allows addition of a custom vocabulary as an input parameter.*\n",
    "\n",
    "## Configure training setup\n",
    "\n",
    "* Split data into train and test using a random shuffle\n",
    "* load desired classifier. In this case, Logistic Regression.\n",
    "* Setup grid search for hyperparameter optimization and train model. Edit the hyper parameter range to search over as per your model.\n",
    "* Fit models to train set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create explainer object that contains defaault glassbox classfier and explanation methods\n",
    "explainer = LinearTextExplainer()\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_str, ylabels, train_size=0.8, test_size=0.2)\n",
    "y_train = label_encoder.fit_transform(y_train)\n",
    "y_test = label_encoder.transform(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"X_train shape =\" + str(X_train.shape))\n",
    "print(\"y_train shape =\" + str(y_train.shape))\n",
    "print(\"X_train data structure = \" + str(type(X_train)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Note:\n",
    "* *Set the parameters using cross-validation*\n",
    "* *Below listed hyperparamters are selected by searching over a larger space.*\n",
    "* *These apply speciically to this instance of the logistic regression model and mnli dataset.*\n",
    "* *'Multinomial' setup was found to be better than 'one-vs-all' across the board*\n",
    "* *Default 'liblinear' solver is not supported for 'multinomial' model setup*\n",
    "* *For a different model or dataset, set the range as appropriate using the hyperparam_range argument in the train method* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier, best_params = explainer.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "* Obtain best model and corresponding hyper parameters\n",
    "* Report Accuracy, F1 score, Precision and Recall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtain best classifier and hyper params\n",
    "print(\"best classifier: \" + str(best_params))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Report performance metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_accuracy = classifier.score(X_test, y_test, sample_weight=None)\n",
    "print(\"accuracy = \" + str(mean_accuracy * 100) + \"%\")\n",
    "y_pred = classifier.predict(X_test)\n",
    "[precision, recall, fscore, support] = precision_recall_fscore_support(y_test, y_pred,average='macro')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Capture metrics for integration testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glue(\"accuracy\", mean_accuracy)\n",
    "glue(\"precision\", precision)\n",
    "glue(\"recall\", recall)\n",
    "glue(\"f1\", fscore)\n",
    "print(\"[precision, recall, fscore, support] = \" + str([precision, recall, fscore, support]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute global importances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize the most and least important words for each class over the entire dataset\n",
    "Choose which label's importances to visualize "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The class names are as follows\")\n",
    "print(label_encoder.classes_)\n",
    "label_name = \"fiction\"\n",
    "\n",
    "# Obtain the top feature ids for the selected class label.\n",
    "# Map top features back to words.\n",
    "explainer.preprocessor.labelEncoder = label_encoder\n",
    "top_words, top_importances = explainer.explain_global(label_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute local importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain the top feature ids for the selected class label.\n",
    "# Map top features back to words.\n",
    "# top_words, top_importances = explainer.explain_global(label_name)\n",
    "\n",
    "# Enter any document & label pair that needs to be interpreted\n",
    "document = \"I travelled to the beach. I took the train. I saw faries, dragons and elves\"\n",
    "\n",
    "# Obtain the top feature ids for the selected class label\n",
    "word_importances, parsed_sentence = explainer.explain_local(document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize local feature importances as a heatmap over words in the document\n",
    "explainer.visualize(word_importances, parsed_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (interpret-text)",
   "language": "python",
   "name": "interpret-text-kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
