{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Copyright (c) Microsoft Corporation. All rights reserved.*                 \n",
    "*Licensed under the MIT License.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interpreting Bag of Words Text Classification models\n",
    "\n",
    "This notebook show a set by step implementation of traditional machine algorithms with a bag of words representation as an interpretable model using feature importances.\n",
    "\n",
    "###### Note:\n",
    "* This example walks through a logistic regression baseline with a simple Bag of Words encoding. However, any model that follows sklearn's classifier API should be supported natively or with minimal tweaking.\n",
    "* The interpreter supports interpretations using either coefficients associated with linear models or feature importances associated with ensemble models.\n",
    "* For larger text datasets, the classifier relies on scipy's sparse representations to keep the dataset in memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import libraries\n",
    "import sys\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy\n",
    "import string\n",
    "from scrapbook.api import glue\n",
    "# sklearn\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.base import TransformerMixin\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.base import TransformerMixin\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "# spacy and nlp recipes\n",
    "import spacy\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "from spacy.lang.en import English\n",
    "from utils_nlp.dataset.multinli import load_pandas_df\n",
    "# for visualizations\n",
    "import matplotlib.pyplot as plt\n",
    "import html\n",
    "from IPython.core.display import display, HTML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "The notebook is built on features made available by [Sci-kit Learn](https://scikit-learn.org/stable/) and [spacy](https://spacy.io/) for easier compatibiltiy with popular tookits.                     \n",
    "The notebook also relies on the pip installable \"utils_nlp.dataset.multinli\" package from Microsoft's [NLP-recipes](https://github.com/microsoft/nlp-recipes/tree/master/utils_nlp) open source package for dataloading purposes.               \n",
    "\n",
    "\n",
    "### An Overview\n",
    "\n",
    "The [Bag of Words](https://en.wikipedia.org/wiki/Bag-of-words_model) allows a 1:1 mapping from the individual words to their respective frequencies in the Document-term matrix.\n",
    "\n",
    "We use spacy's medium language [model](https://spacy.io/models/en#en_core_web_md) that's trained on common crawl for text preprocessing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set configuration parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "SPACY_LANG_MODEL = 'en_core_web_md'\n",
    "DATA_FOLDER = './temp'\n",
    "TRAIN_SIZE = 0.8\n",
    "TEST_SIZE = 0.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = load_pandas_df(DATA_FOLDER, \"train\")\n",
    "df = df[df[\"gold_label\"]==\"neutral\"]  # get unique sentences\n",
    "\n",
    "# fetch documents and labels from data frame\n",
    "X_str = df['sentence1'] # the document we want to analyze\n",
    "ylabels = df['genre'] # the labels, or answers, we want to test against"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure preprocessessing setup\n",
    "\n",
    "Each text document in the dataset under goes the following preprocessing steps during conversion into tokens:\n",
    "\n",
    "* Use spacy's English parser to parse document.\n",
    "* Convert to lower case.\n",
    "* Strip off white space surrounding each word.\n",
    "* [lemmatize](https://en.wikipedia.org/wiki/Lemmatisation) based on spacy's language model.\n",
    "* Remove stop words and punctuation to obtain tokens."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create list of stopwords and punctuation marks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(SPACY_LANG_MODEL)\n",
    "stop_words = spacy.lang.en.stop_words.STOP_WORDS\n",
    "punctuations = string.punctuation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = English()\n",
    "\n",
    "# configure tokenizer function\n",
    "# if keep_idx is true, allows inverse mapping back to text from tokens, for future text highlighting.\n",
    "def spacy_tokenizer(sentence, keep_idx = False):\n",
    "    # Creating our token object, which is used to create documents with linguistic annotations.\n",
    "    mytokens = parser(sentence)\n",
    "        \n",
    "    # Lemmatizing each token, removing blank space and converting each token into lowercase\n",
    "    mytokens = [ word.lemma_.lower().strip() if word.lemma_ != \"-PRON-\" else word.lower_ for word in mytokens ]\n",
    "    \n",
    "    # Removing stop words\n",
    "    if keep_idx is True:\n",
    "        mytokens = [ word if word not in stop_words and word not in punctuations else \"empty_token\" for word in mytokens]\n",
    "        return mytokens\n",
    "    else:\n",
    "        mytokens = [ word for word in mytokens if word not in stop_words and word not in punctuations ]\n",
    "        return mytokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess training data and labels and cast into correct format\n",
    "\n",
    "* Count tokens and construct sparse term document matrix.\n",
    "* Vectorize tokens into counts.\n",
    "* Convert labels from strings into integers.\n",
    "\n",
    "###### Note: Vocabulary\n",
    "\n",
    "* *The vocabulary is compiled from the dataset. This means that any word that does not appear in the Dataset is not a part of the vocabulary. This also means that the vocabulary varies with the dataset.*           \n",
    "* *Every word that appear one or more times is considered to be part of the vocab.*\n",
    "* *However, The sklearn countvectorizer allows addition of a custom vocabulary as an input parameter.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert tokens to BOW count vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "countvectorizer = CountVectorizer(tokenizer = spacy_tokenizer, ngram_range=(1,1))\n",
    "X_vec = countvectorizer.fit_transform(X_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encode labels as numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labelencoder = LabelEncoder()\n",
    "# convert from pandas dataframe to ndarray\n",
    "ylabels = np.asarray(ylabels[:]).reshape(-1,1)\n",
    "ylabels = labelencoder.fit_transform(ylabels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure training setup\n",
    "\n",
    "* Split data into train and test using a random shuffle\n",
    "* load desired classifier. In this case, Logistic Regression.\n",
    "* Setup grid search for hyperparameter optimization and train model. Edit the hyper parameter range to search over as per your model.\n",
    "* Fit models to train set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into training and test set \n",
    "X_train, X_test, y_train, y_test = train_test_split(X_vec,ylabels, train_size = TRAIN_SIZE, \n",
    "                                                    test_size = TEST_SIZE, random_state=0)\n",
    "print(\"X_train shape =\" + str(X_train.shape))\n",
    "print(\"y_train shape =\" + str(y_train.shape))\n",
    "print(\"X_train data structure = \"+ str(type(X_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the parameters using cross-validation\n",
    "tuned_parameters = [{'solver': ['saga'],'multi_class': ['multinomial'], 'C': [10**4]}]\n",
    "\n",
    "model =  LogisticRegression()\n",
    "classifier_CV = GridSearchCV(model, tuned_parameters, cv=3, scoring='accuracy')\n",
    "classifier_CV.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "\n",
    "* Obtain best model and corresponding hyper parameters\n",
    "* report Accuracy, F1 score, Precision and Recall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtain best classifier and hyper params\n",
    "classifier = classifier_CV.best_estimator_\n",
    "print(\"best classifier: \" + str(classifier_CV.best_params_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "report performance metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_accuracy = classifier.score(X_test, y_test, sample_weight=None)\n",
    "print(\"accuracy = \" + str(mean_accuracy*100) + \"%\")\n",
    "y_pred = classifier.predict(X_test)\n",
    "[precision,recall,fscore,support] = precision_recall_fscore_support(y_test, y_pred,average='macro')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glue(\"accuracy\", mean_accuracy)\n",
    "glue(\"precision\", precision)\n",
    "glue(\"recall\", recall)\n",
    "glue(\"f1\", fscore)\n",
    "\n",
    "print (\"[precision,recall,fscore,support] = \" + str([precision,recall,fscore,support]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute global importances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize the most and least important words for each class over the entire dataset\n",
    "Choose which label's importances to visualize "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The class names are as follows\")\n",
    "print(labelencoder.classes_)\n",
    "label_name = \"fiction\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obtain the top feature ids for the selected class label.           \n",
    "Map top features back to words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_coefs_all =  classifier.coef_\n",
    "label_coefs = label_coefs_all[labelencoder.transform([label_name]),:]\n",
    "sorting_ids = (np.flip(np.argsort(np.abs(label_coefs)))).flatten()\n",
    "top_ids = sorting_ids[0:20] # view top 20 features per label\n",
    "top_words = [countvectorizer.get_feature_names()[i] for i in top_ids]\n",
    "top_importances = [label_coefs[0,i] for i in top_ids]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the feature importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (14,7))\n",
    "plt.title(\"most important words for class label: \" + str(label_name), fontsize = 18 )\n",
    "plt.bar(range(len(top_importances)), top_importances,\n",
    "       color=\"r\", align=\"center\")\n",
    "plt.xticks(range(len(top_importances)), top_words, rotation=60, fontsize = 18)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute local importances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Enter any document & label pair that needs to be interpreted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "document = \"I travelled to the beach. I took the train. I saw faries, dragons and elves\"\n",
    "label_name = \"travel\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obtain the top feature ids for the selected class label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_coefs = classifier.coef_[labelencoder.transform([label_name]),:]\n",
    "parsed_sentence = []\n",
    "for i in parser(document):\n",
    "    parsed_sentence += [str(i)]\n",
    "parsed_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_tokens = spacy_tokenizer(document, keep_idx = True)\n",
    "word_ids = [None if word is 'empty_token' else countvectorizer.vocabulary_.get(word) for word in doc_tokens]\n",
    "word_importances = [0 if idx==None else label_coefs[0,idx] for idx in word_ids]\n",
    "word_importances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize local feature importances as a heatmap over words in the document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prevent special characters like & and < to cause the browser to display something other than what you intended.\n",
    "def html_escape(text):\n",
    "    return html.escape(text)\n",
    "\n",
    "max_alpha = 0.5\n",
    "highlighted_text = []\n",
    "for i,word in enumerate(parsed_sentence):\n",
    "    weight = word_importances[i]\n",
    "    if weight > 0:\n",
    "        highlighted_text.append('<span style=\"background-color:rgba(135,206,250,' + str(abs(weight) / max_alpha) +\n",
    "                                ');\">' + html_escape(word) + '</span>')\n",
    "    elif weight < 0:\n",
    "        highlighted_text.append('<span style=\"background-color:rgba(250,0,0,' + str(abs(weight) / max_alpha) +\n",
    "                                ');\">' + html_escape(word) + '</span>')\n",
    "    else:\n",
    "        highlighted_text.append(word)\n",
    "\n",
    "highlighted_text = ' '.join(highlighted_text)\n",
    "display(HTML(highlighted_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
