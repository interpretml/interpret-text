{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eac1aaf-889a-4b82-b8d2-3d8be01dea1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "import argparse\n",
    "import torch\n",
    "import json\n",
    "import transformers \n",
    "from model_lib.hf_tooling import HF_LM\n",
    "from tqdm import tqdm\n",
    "from hooks import *\n",
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "from easydict import EasyDict as edict\n",
    "\n",
    "from rare_knowledge.collection import *\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import average_precision_score, roc_auc_score, precision_recall_curve, auc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e7500d6-cd69-4c9e-b2f0-8e0be3d605ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_sub_list(sl,l, offset=0):\n",
    "    sll=len(sl)\n",
    "    for ind in (i for i,e in enumerate(l) if e==sl[0]):\n",
    "        if ind < offset:\n",
    "            continue\n",
    "        if l[ind:ind+sll]==sl:\n",
    "            return ind,ind+sll-1\n",
    "\n",
    "def find_within_text(prompt, parts, tokenizer):\n",
    "    \"\"\"\n",
    "    A function that identifies the indices of tokens of a part of the prompt. \n",
    "    By default we use the first occurence. \n",
    "    \"\"\"\n",
    "    prompt_tokens = tokenizer.encode(prompt)\n",
    "    part_tokens = [tokenizer.encode(p)[2:] for p in parts]\n",
    "    part_token_indices = [find_sub_list(pt, prompt_tokens) for pt in part_tokens]\n",
    "    return part_token_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21b4580f-0a38-4fbc-9dab-5c91765b0413",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-13b-hf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5809dd7-8f09-4724-9027-aaf236e601ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_metrics(y, score):\n",
    "    ## TODO: Add risk @ top 20% coverage\n",
    "    ## TODO: Add risk @ bottom 20%\n",
    "    roc_auc = roc_auc_score(y, score)\n",
    "    precision, recall, _ = precision_recall_curve(y, score)\n",
    "    pr_auc = auc(recall, precision)\n",
    "    bottom20_idx = np.argsort(score)[:int(score.shape[0]*0.2)]\n",
    "    top20_idx =  np.argsort(-score)[:int(score.shape[0]*0.2)]\n",
    "    risk_at_top20 = 1-y[top20_idx].mean()\n",
    "    risk_at_bottom20 = 1-y[bottom20_idx].mean()\n",
    "    accuracy = ((score >= 0.5) == y).mean()\n",
    "    return {r\"AUROC$\\textcolor{Green}{\\mathbf{(\\Uparrow)}}$\": roc_auc, \n",
    "            r\"$\\text{Risk}_{[q_{0.8}, q_{1.0}]}\\textcolor{Red}{\\mathbf{(\\Downarrow)}}$\": risk_at_top20, \n",
    "            r\"$\\text{Risk}_{[q_{0.0}, q_{0.2}]}\\textcolor{Green}{\\mathbf{(\\Uparrow)}}$\":risk_at_bottom20}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50303a3a-98eb-43f9-a921-28cd2e7a5405",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def extract_predictors(records):\n",
    "    norms = records.norms\n",
    "    att_weights = records.att_ws\n",
    "    indices = np.arange(len(norms))\n",
    "    correctness = []\n",
    "    predictors = defaultdict(list)\n",
    "    for token_head_norms, idx in zip(norms, indices):\n",
    "        max_norms, max_weights = [], [] \n",
    "        for constraint_idx, constraint_norms in enumerate(token_head_norms):\n",
    "            prompt = records[\"prompt\"][idx]\n",
    "            filler_indices = find_within_text(prompt, [records[\"name\"][idx][constraint_idx]], tokenizer)[0]\n",
    "            constraint_att_weights = att_weights[idx][constraint_idx]\n",
    "            max_norms.append(constraint_norms[:, :, 0, :].max(axis=2).reshape(-1))\n",
    "            max_weights.append(constraint_att_weights[:, :, 0, :].max(axis=2).reshape(-1))\n",
    "        \n",
    "        predictors[r\"$||a_{C,g}^{\\ell, [h]}||$\"].append(max_norms)\n",
    "        predictors[r\"$||A_{C,g}^{\\ell, [h]}||$\"].append(max_weights)\n",
    "        predictors[r\"$\\hat{P}(\\hat{Y}|X)$\"].append([records[\"pred_logprob\"][idx].reshape((-1))]*2)\n",
    "        \n",
    "    predictors = {k: np.array(v) for k,v in predictors.items()}    \n",
    "    return predictors        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86a0b018-9a09-46d4-8208-e5ed6e182163",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_predictors = defaultdict(dict)\n",
    "all_labels = defaultdict(dict)\n",
    "from sklearn.model_selection import train_test_split \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "133527e8-4d70-42d8-bd7f-ec4a0432e9cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO: Train something `per` constraint.\n",
    "constraint_names = {\"word_startend\": [r\"\\textit{starts with}\", r\"\\textit{ends with}\"],\n",
    "                    \"senator_multiconstraint\": [r\"\\textit{represented state}\", r\"\\textit{alma mater}\"],\n",
    "                   \"movie_awards\": [r\"\\textit{directed by}\", r\"\\textit{won award}\"],\n",
    "                    \"nobel_city\":  [r\"\\textit{won Nobel}\", r\"\\textit{born in city}\"],\n",
    "                    \"books\": [r\"\\textit{author}\", r\"\\textit{published year}\"]\n",
    "                   }\n",
    "data_pretty = {\n",
    "    \"books\": \"Books\",\n",
    "    \"word_startend\": \"Words\",\n",
    "    \"movie_awards\": \"Movies\",\n",
    "    \"senator_multiconstraint\": \"Senators\",\n",
    "    \"nobel_city\": \"Nobel Winner\",\n",
    "              }\n",
    "\n",
    "result_records = []\n",
    "for data_name in data_pretty:\n",
    "    for model_size in [\"7b\", \"13b\", \"70b\"]:\n",
    "        print(model_size, data_name)\n",
    "        filename = f\"/home/t-merty/mounts/sandbox-mert/Llama-2-{model_size}-hf_{data_name}_localized_track.pkl\"\n",
    "        y_file =  f\"/home/t-merty/mounts/sandbox-mert/multiconstraint-labels/Llama-2-{model_size}-hf_{data_name}_verified.npy\"\n",
    "        if not os.path.exists(filename):\n",
    "            print(filename)\n",
    "            continue\n",
    "        records_to_save = edict(pickle.load(open(filename, \"rb\")))\n",
    "        records = records_to_save\n",
    "        predictors = extract_predictors(records)\n",
    "        y = np.load(y_file)\n",
    "        print(y.mean(), y_file)\n",
    "        predictors[\"Majority\"] = np.zeros_like(y)\n",
    "        predictors[\"Majority\"][:, 0] = int(y.mean(axis=0)[0] >= 0.5)\n",
    "        predictors[\"Majority\"][:, 1] = int(y.mean(axis=0)[1] >= 0.5)\n",
    "\n",
    "        all_labels[model_size][data_name] = y\n",
    "        all_predictors[model_size][data_name] = predictors\n",
    "\n",
    "        train_idx, test_idx = train_test_split(np.arange(predictors[r\"$\\hat{P}(\\hat{Y}|X)$\"].shape[0]), test_size=0.5)\n",
    "        for constraint_idx in range(y.shape[1]):\n",
    "            for predictor in predictors:\n",
    "                y_train = y[train_idx, constraint_idx]\n",
    "                y_test = y[test_idx, constraint_idx]\n",
    "                X_train = predictors[predictor][train_idx, constraint_idx].reshape((y_train.shape[0], -1))\n",
    "                X_test = predictors[predictor][test_idx, constraint_idx].reshape((y_test.shape[0], -1))\n",
    "                lr = LogisticRegression(max_iter=10000)\n",
    "                lr.fit(X_train, y_train)\n",
    "                score = lr.predict_proba(X_test)[:, 1]\n",
    "                metrics = get_metrics(y_test, score)\n",
    "                #print(predictor, metrics, data_name)\n",
    "                result_records.append({\"Model Size\": model_size, \n",
    "                                       \"Data\": rf\"{data_pretty[data_name]}\", \n",
    "                                       \"BaseRate\": y_test.mean(), \n",
    "                                       \"Predictor\": predictor, \n",
    "                                       \"Constraint\": constraint_names[data_name][constraint_idx],\n",
    "                                       **metrics})\n",
    "df_results = pd.DataFrame(result_records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ab9f80e-1286-4ee5-95a3-e60d53b50077",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results = pd.DataFrame(result_records)\n",
    "df_results = df_results[df_results.Predictor != r\"$||a_{C,g}^{\\ell, [h]}||$\"]\n",
    "pivot_df = df_results.pivot_table(index=['Model Size', 'Data', 'Constraint', \"BaseRate\"], \n",
    "                                                           columns='Predictor', \n",
    "                                                           values=list(df_results.columns[4:]), aggfunc='first')\n",
    "pivot_df.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b5a8595-b0f2-4bf9-a818-9ec39878c61b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_latex_with_multicolumns(df, metrics, predictors, plot_header=False):\n",
    "    # Generate initial LaTeX table\n",
    "    latex_str = df.to_latex(index=False, float_format=lambda x: f\"${x:.2f}$\")\n",
    "\n",
    "    \n",
    "    # Find the line with the headers\n",
    "    lines = latex_str.split('\\n')\n",
    "    header_line_idx = 2\n",
    "    \n",
    "    # Create multicolumn headers and new column format\n",
    "    multicolumn_headers = ' & '.join([f'\\\\multicolumn{{{len(predictors)}}}{{c||}}{{{metric}}}' for metric in metrics])\n",
    "    new_col_format = \"|c|c|c|c|\" + \"|c|c|c|c|c|\" * len(metrics)\n",
    "    \n",
    "    # Insert multicolumn headers and adjust column format\n",
    "    lines[header_line_idx] = f' Model & Data & Constraint & Model Success & {multicolumn_headers} \\\\\\\\'\n",
    "    lines[0] = f'\\\\begin{{tabular}}{{{new_col_format}}}'\n",
    "    lines.insert(header_line_idx+1,\"\\\\midrule\")\n",
    "    lines.insert(0, \"\\\\begin{adjustbox}{width=\\\\textwidth}\")\n",
    "    lines.insert(-1, \"\\\\end{adjustbox}\")\n",
    "    if plot_header:\n",
    "        return '\\n'.join(lines[:-3])\n",
    "    else:\n",
    "        return '\\n'.join(lines[6:-3])\n",
    "\n",
    "# List of metrics\n",
    "metrics = list()\n",
    "for v in pivot_df.columns.values[4:]:\n",
    "    if v[0] not in metrics:\n",
    "        metrics.append(v[0])\n",
    "predictors = df_results.Predictor.unique()\n",
    "# Generate LaTeX table with multicolumn headers\n",
    "for j, model_size in enumerate([\"7b\", \"13b\", \"70b\"]):\n",
    "    sub_df = pivot_df[pivot_df['Model Size'] == model_size]\n",
    "    modified_latex_str = generate_latex_with_multicolumns(sub_df, metrics, predictors, plot_header = (j==0))\n",
    "    print(modified_latex_str)\n",
    "print(r\"\\end{tabular}\")\n",
    "print(r\"\\end{adjustbox}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28cc46bc-bfc9-40ac-b1a3-476587af0d09",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(predictors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5985baa5-b953-49ea-baf4-2e5b72d2cfef",
   "metadata": {},
   "outputs": [],
   "source": [
    "overall_records = []\n",
    "for model_size, model_predictors in all_predictors.items():\n",
    "    for data_name, predictors in model_predictors.items():\n",
    "        y = all_labels[model_size][data_name]\n",
    "        \n",
    "        predictors = all_predictors[model_size][data_name]\n",
    "\n",
    "        overall_predictors = defaultdict(list)\n",
    "        \n",
    "        train_idx, test_idx = train_test_split(np.arange(y.shape[0]), test_size=0.5)\n",
    "        y_overall = np.all(y, axis=1)[test_idx]\n",
    "        for constraint_idx in range(y.shape[1]):\n",
    "            for predictor in predictors:\n",
    "                print(predictor)\n",
    "                if predictor in [r\"$\\hat{P}(\\hat{Y}|X)$\"]:\n",
    "                    overall_predictors[predictor].append(predictors[predictor][test_idx, 0])\n",
    "                    \n",
    "                else:\n",
    "                    y_train = y[train_idx, constraint_idx]\n",
    "                    y_test = y[test_idx, constraint_idx]\n",
    "                    X_train = predictors[predictor][train_idx, constraint_idx].reshape((y_train.shape[0], -1))\n",
    "                    X_test = predictors[predictor][test_idx, constraint_idx].reshape((y_test.shape[0], -1))\n",
    "                    lr = LogisticRegression(max_iter=10000)\n",
    "                    lr.fit(X_train, y_train)\n",
    "                    y_pred_proba = lr.predict_proba(X_test)[:, 1]\n",
    "                    overall_predictors[predictor].append(y_pred_proba)\n",
    "                    \n",
    "        for predictor in overall_predictors:\n",
    "            if predictor not in [r\"$\\hat{P}(\\hat{Y}|X)$\"]:\n",
    "                y_pred = overall_predictors[predictor][0]*overall_predictors[predictor][1]\n",
    "                print(y_pred.shape, predictor)\n",
    "                metrics = get_metrics(y_overall, y_pred)\n",
    "            else:\n",
    "                metrics = get_metrics(y_overall, -overall_predictors[predictor][0])\n",
    "            \n",
    "            overall_records.append({\"Model Size\": model_size, \n",
    "                   \"Data\": rf\"{data_pretty[data_name]}\", \n",
    "                   \"BaseRate\": y_overall.mean(), \n",
    "                   \"Predictor\": predictor, \n",
    "                   \"Constraint\": \"overall\",\n",
    "                   **metrics})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56a70527-00ae-4887-8f2c-2a0677ec5667",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_overall_results = pd.DataFrame(overall_records)\n",
    "pivot_df = df_overall_results.pivot_table(index=['Model Size', 'Data', 'Constraint', \"BaseRate\"], \n",
    "                                                           columns='Predictor', \n",
    "                                                           values=list(df_results.columns[4:]), aggfunc='first')\n",
    "pivot_df.reset_index(inplace=True)\n",
    "\n",
    "def generate_latex_with_multicolumns(df, metrics, predictors, plot_header=False):\n",
    "    # Generate initial LaTeX table\n",
    "    latex_str = df.to_latex(index=False, float_format=lambda x: f\"${x:.2f}$\")\n",
    "\n",
    "    \n",
    "    # Find the line with the headers\n",
    "    lines = latex_str.split('\\n')\n",
    "    header_line_idx = 2\n",
    "    \n",
    "    # Create multicolumn headers and new column format\n",
    "    multicolumn_headers = ' & '.join([f'\\\\multicolumn{{{len(predictors)}}}{{c||}}{{{metric}}}' for metric in metrics])\n",
    "    new_col_format = \"|c|c|c|c|\" + \"|c|c|c|c|c|\" * len(metrics)\n",
    "    \n",
    "    # Insert multicolumn headers and adjust column format\n",
    "    lines[header_line_idx] = f' Model & Data & Constraint & Model Success & {multicolumn_headers} \\\\\\\\'\n",
    "    lines[0] = f'\\\\begin{{tabular}}{{{new_col_format}}}'\n",
    "    lines.insert(header_line_idx+1,\"\\\\midrule\")\n",
    "    lines.insert(0, \"\\\\begin{adjustbox}{width=\\\\textwidth}\")\n",
    "    lines.insert(-1, \"\\\\end{adjustbox}\")\n",
    "    if plot_header:\n",
    "        return '\\n'.join(lines[:-3])\n",
    "    else:\n",
    "        return '\\n'.join(lines[6:-3])\n",
    "\n",
    "# List of metrics\n",
    "metrics = list()\n",
    "for v in pivot_df.columns.values[4:]:\n",
    "    if v[0] not in metrics:\n",
    "        metrics.append(v[0])\n",
    "predictors = df_results.Predictor.unique()\n",
    "# Generate LaTeX table with multicolumn headers\n",
    "for j, model_size in enumerate([\"7b\", \"13b\", \"70b\"]):\n",
    "    sub_df = pivot_df[pivot_df['Model Size'] == model_size]\n",
    "    modified_latex_str = generate_latex_with_multicolumns(sub_df, metrics, predictors, plot_header = (j==0))\n",
    "    print(modified_latex_str)\n",
    "print(r\"\\end{tabular}\")\n",
    "print(r\"\\end{adjustbox}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
