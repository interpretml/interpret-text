{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eac1aaf-889a-4b82-b8d2-3d8be01dea1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "import argparse\n",
    "import torch\n",
    "import transformers \n",
    "from model_lib.hf_tooling import HF_LM\n",
    "from tqdm import tqdm\n",
    "from hooks import *\n",
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "from easydict import EasyDict as edict\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.feature_selection import f_regression\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from rare_knowledge.collection import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e7500d6-cd69-4c9e-b2f0-8e0be3d605ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import average_precision_score, roc_auc_score, precision_recall_curve, auc\n",
    "\n",
    "def find_sub_list(sl,l, offset=0):\n",
    "    sll=len(sl)\n",
    "    for ind in (i for i,e in enumerate(l) if e==sl[0]):\n",
    "        if ind < offset:\n",
    "            continue\n",
    "        if l[ind:ind+sll]==sl:\n",
    "            return ind,ind+sll-1\n",
    "\n",
    "def find_within_text(prompt, parts, tokenizer):\n",
    "    \"\"\"\n",
    "    A function that identifies the indices of tokens of a part of the prompt. \n",
    "    By default we use the first occurence. \n",
    "    \"\"\"\n",
    "    prompt_tokens = tokenizer.encode(prompt)\n",
    "    part_tokens = [tokenizer.encode(p)[2:] for p in parts]\n",
    "    part_token_indices = [find_sub_list(pt, prompt_tokens) for pt in part_tokens]\n",
    "    return part_token_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21b4580f-0a38-4fbc-9dab-5c91765b0413",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-13b-hf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c96ece93-4f69-4cf8-a7ef-4e22bb3ee522",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "def extract_predictors(records):\n",
    "    norms = records.norms\n",
    "    att_weigts = records.att_ws\n",
    "    indices = np.arange(len(norms))\n",
    "    corr = []\n",
    "    predictors = defaultdict(list)\n",
    "    \n",
    "    for token_head_norms, idx in zip(norms, indices):\n",
    "        prompt = records[\"prompt\"][idx]\n",
    "        filler_indices = find_within_text(prompt, [\" \"+records[\"name\"][idx]], tokenizer)[0]\n",
    "        max_norms = token_head_norms[:].sum(axis=(1)).max(axis=1)\n",
    "        max_weights = att_weigts[idx].max(axis=1).max(axis=1)\n",
    "        corr.append(int(records[\"gt_logprob\"][idx]==records[\"pred_logprob\"][idx]))\n",
    "        predictors[r\"$||a_{c,g}^{\\ell, [h]}||$\"].append(token_head_norms[:].max(axis=2).reshape((1, -1)))\n",
    "        #predictors[r\"$||a_{C,g}^{\\ell, [h]}||$\"].append(token_head_norms[:, :, -1].reshape((1, -1)))\n",
    "        predictors[r\"$||A_{c,g}^{\\ell, [h]}||$\"].append(att_weigts[idx].max(axis=2).reshape((1, -1)))\n",
    "        #predictors[r\"$||A_{C,g}^{\\ell, [h]}||$\"].append(att_weigts[idx][:, :, -1].reshape((1, -1)))\n",
    "        \n",
    "    predictors = {k: np.array(v) for k,v in predictors.items()}\n",
    "    predictors[r\"$\\hat{P}(\\hat{Y}|X)$\"] = np.array(records[\"pred_logprob\"])\n",
    "    y = np.array(corr)\n",
    "    predictors[\"Majority\"] = np.ones(y.shape[0]) if np.mean(y) > 0.5 else np.zeros(y.shape[0])\n",
    "    return predictors, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "472149f6-ee3c-4464-ba8a-d67c63fa2f76",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_metrics(y, score):\n",
    "    ## TODO: Add risk @ top 20% coverage\n",
    "    ## TODO: Add risk @ bottom 20%\n",
    "    roc_auc = roc_auc_score(y, score)\n",
    "    precision, recall, _ = precision_recall_curve(y, score)\n",
    "    pr_auc = auc(recall, precision)\n",
    "    bottom20_idx = np.argsort(score)[:int(score.shape[0]*0.2)]\n",
    "    top20_idx =  np.argsort(-score)[:int(score.shape[0]*0.2)]\n",
    "    risk_at_top20 = 1-y[top20_idx].mean()\n",
    "    risk_at_bottom20 = 1-y[bottom20_idx].mean()\n",
    "\n",
    "    accuracy = (y == (score >= 0.5)).mean()\n",
    "    return {r\"AUROC$\\textcolor{Green}{\\mathbf{(\\Uparrow)}}$\": roc_auc, \n",
    "            r\"$\\text{Risk}_{[q_{0.8}, q_{1.0}]}\\textcolor{Red}{\\mathbf{(\\Downarrow)}}$\": risk_at_top20, \n",
    "            r\"$\\text{Risk}_{[q_{0.0}, q_{0.2}]}\\textcolor{Green}{\\mathbf{(\\Uparrow)}}$\":risk_at_bottom20}\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0863e960-fa1f-4c92-95b0-c2858f5b08a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "data_pretty = {\"songs\": \"Songs\", \"movies\": \"Movies\", \"football_teams\": \"Football Teams\", \"basketball_players\": \"Basketball Players\",\n",
    "              \"schools\" : \"Schools\", \"counterfact\": \"CounterFact\",\n",
    "                'counterfact_mother_tongue': 'CounterFact',\n",
    "                'counterfact_official_religion': 'CounterFact',\n",
    "                'counterfact_twin_city': 'CounterFact',\n",
    "                'counterfact_capital': 'CounterFact',\n",
    "                'counterfact_citizenship': 'CounterFact',\n",
    "                'counterfact_language_used': 'CounterFact',\n",
    "                'counterfact_original_language': 'CounterFact',\n",
    "                'counterfact_headquarter_location': 'CounterFact'\n",
    "              }\n",
    "constraint_names = {\"songs\": r\"\\textit{performed by}\",\n",
    "                    \"movies\": r\"\\textit{directed by}\",\n",
    "                    \"football_teams\": r\"\\textit{founded in the year}\",\n",
    "                    \"basketball_players\": r\"\\textit{born in the year}\",\n",
    "                    \"schools\": r\"\\textit{founded in the year}\",\n",
    "                    \"counterfact\": r\"mixed\",\n",
    "                    'counterfact_mother_tongue': r\"\\textit{Mother Tongue}\",\n",
    "                    'counterfact_official_religion': r\"\\textit{Official Religion}\",\n",
    "                    'counterfact_twin_city': r\"\\textit{Twin City}\",\n",
    "                    'counterfact_capital': r\"\\textit{Capital}\",\n",
    "                    'counterfact_citizenship': r\"\\textit{Citizenship}\",\n",
    "                    'counterfact_language_used': r\"\\textit{Language Used}\",\n",
    "                    'counterfact_original_language': r\"\\textit{Original Language}\",\n",
    "                    'counterfact_headquarter_location': r\"\\textit{Headquarter Location}\"\n",
    "                   }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26fc978f-2e3d-45be-8a25-fcfbed778812",
   "metadata": {},
   "outputs": [],
   "source": [
    "for data_name in [\"basketball_players\", \"songs\", \"movies\", \"football_teams\", \"schools\", \n",
    "                      'counterfact_mother_tongue', \n",
    "                      'counterfact_official_religion', \n",
    "                      'counterfact_citizenship', \n",
    "                      'counterfact_original_language', \n",
    "                      'counterfact_headquarter_location']:\n",
    "    for model_size in [\"7b\", \"13b\", \"70b\"]:\n",
    "        \n",
    "        filename = f\"/home/t-merty/mounts/sandbox-mert/Llama-2-{model_size}-hf_{data_name}_finalrun.pkl\"\n",
    "        if not os.path.exists(filename):\n",
    "            print(filename, \"damn\")\n",
    "        else:\n",
    "            records = edict(pickle.load(open(filename, \"rb\")))\n",
    "            print(data_name, model_size, len(records.prompt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc9f5114-c048-4c4a-905e-951810c4b559",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_predictors = defaultdict(dict)\n",
    "all_labels = defaultdict(dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "133527e8-4d70-42d8-bd7f-ec4a0432e9cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "result_records = []\n",
    "for model_size in [\"7b\", \"13b\", \"70b\"]:\n",
    "    for data_name in [\"basketball_players\", \"songs\", \"movies\", \"football_teams\", \"schools\", \n",
    "                      'counterfact_mother_tongue', \n",
    "                      'counterfact_official_religion', \n",
    "                      'counterfact_citizenship', \n",
    "                      'counterfact_original_language', \n",
    "                      'counterfact_headquarter_location']:\n",
    "        print(model_size, data_name)\n",
    "        filename = f\"/home/t-merty/mounts/sandbox-mert/Llama-2-{model_size}-hf_{data_name}_finalrun.pkl\"\n",
    "        if not os.path.exists(filename):\n",
    "            continue\n",
    "        records_to_save = edict(pickle.load(open(filename, \"rb\")))\n",
    "        records = records_to_save\n",
    "        if \"counterfact\" not in data_name:\n",
    "            filter = (np.array(records.popularity)>=15).nonzero()[0]\n",
    "        else:\n",
    "            filter = (np.array(records.popularity)>=-np.inf).nonzero()[0]\n",
    "        for k, v in records.items():\n",
    "            records[k] = [records_to_save[k][f] for f in filter]\n",
    "        predictors, y = extract_predictors(records)\n",
    "        y = y.reshape((-1))\n",
    "        all_labels[model_size][data_name] = y\n",
    "        all_predictors[model_size][data_name] = predictors\n",
    "        \n",
    "        train_idx, test_idx, y_train, y_test = train_test_split(np.arange(y.shape[0]), y, test_size=0.5, stratify=y)\n",
    "        print(data_name)\n",
    "        \n",
    "        for predictor in predictors:\n",
    "            X_train = predictors[predictor][train_idx].reshape((y_train.shape[0], -1))\n",
    "            X_test = predictors[predictor][test_idx].reshape((y_test.shape[0], -1))\n",
    "            lr = LogisticRegression(max_iter=10000)\n",
    "            lr.fit(X_train, y_train)\n",
    "            score = lr.predict_proba(X_test)[:, 1]\n",
    "            metrics = get_metrics(y_test, score)\n",
    "            print(y_train.shape[0], y_test.shape[0])\n",
    "            print(predictor, metrics)\n",
    "            print()\n",
    "            result_records.append({\"Model Size\": model_size, \n",
    "                                   \"Constraint\": constraint_names[data_name],\n",
    "                                   \"Data\": rf\"{data_pretty[data_name]}\", \n",
    "                                   \"BaseRate\": y_test.mean(), \n",
    "                                   \"Predictor\": predictor, \n",
    "                                   **metrics})\n",
    "df_results = pd.DataFrame(result_records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93532475-8cef-4489-b8f5-ff2e55389172",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_records = []\n",
    "\n",
    "for model_size, model_predictors in all_predictors.items():\n",
    "\n",
    "    for data_name, predictors in model_predictors.items():\n",
    "        y = all_labels[model_size][data_name]\n",
    "\n",
    "        for seed in range(1, 11):\n",
    "            train_idx, test_idx, y_train, y_test = train_test_split(np.arange(y.shape[0]), y, test_size=0.5, stratify=y, random_state=seed)\n",
    "    \n",
    "            for predictor, array in predictors.items():\n",
    "                X_train = array[train_idx].reshape((y_train.shape[0], -1))\n",
    "                X_test = array[test_idx].reshape((y_test.shape[0], -1))\n",
    "                \n",
    "                lr = LogisticRegression(max_iter=10000)\n",
    "                lr.fit(X_train, y_train)\n",
    "                score = lr.predict_proba(X_test)[:, 1]\n",
    "                metrics = get_metrics(test_labels[data_name], score)\n",
    "                print(predictor, metrics)\n",
    "                print(get_metrics(y_train, lr.predict_proba(X_train)[:, 1]))\n",
    "                print()\n",
    "                result_records.append({\"Model Size\": model_size, \n",
    "                                       \"Constraint\": constraint_names[data_name],\n",
    "                                       \"Data\": rf\"{data_pretty[data_name]}\", \n",
    "                                       \"BaseRate\": y_test.mean(), \n",
    "                                       \"Predictor\": predictor, \n",
    "                                       \"Seed\": seed,\n",
    "                                       **metrics})\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "177438b3-cc22-4b74-8251-fdcd7873d23c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results = pd.DataFrame(result_records)\n",
    "df_results = df_results[df_results.Predictor != r\"$||a_{c,g}^{\\ell, [h]}||$\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ab9f80e-1286-4ee5-95a3-e60d53b50077",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pivot table to get desired format\n",
    "pivot_df = df_results.pivot_table(index=['Model Size', 'Data', 'Constraint', \"BaseRate\"], \n",
    "                                                           columns='Predictor', \n",
    "                                                           values=list(df_results.columns[4:]), aggfunc='first')\n",
    "pivot_df.reset_index(inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b5a8595-b0f2-4bf9-a818-9ec39878c61b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_latex_with_multicolumns(df, metrics, predictors, plot_header=False):\n",
    "    # Generate initial LaTeX table\n",
    "    latex_str = df.to_latex(index=False, float_format=lambda x: f\"${x:.2f}$\")\n",
    "\n",
    "    \n",
    "    # Find the line with the headers\n",
    "    lines = latex_str.split('\\n')\n",
    "    header_line_idx = 2\n",
    "    \n",
    "    # Create multicolumn headers and new column format\n",
    "    multicolumn_headers = ' & '.join([f'\\\\multicolumn{{{len(predictors)}}}{{c|}}{{{metric}}}' for metric in metrics])\n",
    "    new_col_format = \"|c|c|c|c|\" + \"|c|c|c|c|\" * len(metrics)\n",
    "    \n",
    "    # Insert multicolumn headers and adjust column format\n",
    "    lines[header_line_idx] = f' Model & Data & Constraint & Model Success & {multicolumn_headers} \\\\\\\\'\n",
    "    lines[0] = f'\\\\begin{{tabular}}{{{new_col_format}}}'\n",
    "    lines.insert(header_line_idx+1,\"\\\\midrule\")\n",
    "    lines.insert(0, \"\\\\begin{adjustbox}{width=\\\\textwidth}\")\n",
    "    lines.insert(-1, \"\\\\end{adjustbox}\")\n",
    "    if plot_header:\n",
    "        return '\\n'.join(lines[:-3])\n",
    "    else:\n",
    "        return '\\n'.join(lines[6:-3])\n",
    "\n",
    "\n",
    "# List of metrics\n",
    "metrics = list()\n",
    "for v in pivot_df.columns.values[4:]:\n",
    "    if v[0] not in metrics:\n",
    "        metrics.append(v[0])\n",
    "predictors = df_results.Predictor.unique()\n",
    "# Generate LaTeX table with multicolumn headers\n",
    "\n",
    "for j, model_size in enumerate([\"7b\", \"13b\", \"70b\"]):\n",
    "    sub_df = pivot_df[pivot_df['Model Size'] == model_size]\n",
    "    modified_latex_str = generate_latex_with_multicolumns(sub_df, metrics, predictors, plot_header = (j==0))\n",
    "    print(modified_latex_str)\n",
    "print(r\"\\end{tabular}\")\n",
    "print(r\"\\end{adjustbox}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bac5e21-b3e4-4ac9-a72f-72dc99fedfd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import average_precision_score, roc_auc_score, precision_recall_curve, auc\n",
    "from kondo import use_style; use_style(palette=\"bmh\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e2899b9-4669-4797-9ea8-64162f2c31e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_context(\"paper\")\n",
    "sns.set_palette(\"deep\")\n",
    "df_results_cp = df_results.copy()\n",
    "df_results_cp[\"AUROC\"] = df_results_cp[r\"AUROC$\\textcolor{Green}{\\mathbf{(\\Uparrow)}}$\"]\n",
    "for metric in [\"AUROC\"]:\n",
    "\n",
    "    df_results_cp[\"DS+C\"] = df_results_cp.apply(lambda x: x[\"Data\"]+\" | \"+x[\"Constraint\"], axis=1)\n",
    "    fig, axs = plt.subplots(1, 3, figsize=(10, 2))\n",
    "    for i, ms in enumerate(df_results_cp[\"Model Size\"].unique()):\n",
    "        df_results_model = df_results_cp[df_results_cp[\"Model Size\"] == ms]\n",
    "        model = f\"Llama-2 {ms.upper()}\"\n",
    "        ax = axs[i]\n",
    "        axs[i].set_title(model)\n",
    "        sns.scatterplot(data=df_results_model, x=metric,y=\"DS+C\",  hue=\"Predictor\", s=52, ax=ax)\n",
    "        if i != 0:\n",
    "            ax.set_yticks([])\n",
    "    \n",
    "        if i != 1:\n",
    "            ax.get_legend().remove()\n",
    "        \n",
    "        if i == 1:\n",
    "            ax.legend(loc=\"upper left\")\n",
    "        \n",
    "        ax.set_ylabel(\"\")\n",
    "    \n",
    "    fig.tight_layout()\n",
    "    fig.savefig(f\"./figures/figure5_{metric}.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "969ef6f7-ef65-4124-994f-8d538fc821d9",
   "metadata": {},
   "source": [
    "## Generalization Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7374af38-699b-4ae7-8159-5f933aafc86e",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_records = []\n",
    "for model_size, model_predictors in all_predictors.items():\n",
    "    train_data = defaultdict(list)\n",
    "    train_labels = []\n",
    "    test_data = defaultdict(dict)\n",
    "    test_labels = dict()\n",
    "    for data_name, predictors in model_predictors.items():\n",
    "        y = all_labels[model_size][data_name]\n",
    "        train_idx, test_idx, y_train, y_test = train_test_split(np.arange(y.shape[0]), y, test_size=0.5, stratify=y)\n",
    "        train_labels.append(y_train)\n",
    "        for predictor, array in predictors.items():\n",
    "            train_data[predictor].append(array[train_idx].reshape((y_train.shape[0], -1)))\n",
    "            test_data[predictor][data_name] = array[test_idx].reshape((y_test.shape[0], -1))\n",
    "            test_labels[data_name] = y_test\n",
    "            \n",
    "            \n",
    "    for predictor in train_data:\n",
    "        X_train = np.concatenate(train_data[predictor])\n",
    "        lr = LogisticRegression(max_iter=10000)\n",
    "        y_train = np.concatenate(train_labels)\n",
    "        lr.fit(X_train, y_train)\n",
    "\n",
    "        for data_name in test_data[predictor]:\n",
    "            X_test = test_data[predictor][data_name]\n",
    "            score = lr.predict_proba(X_test)[:, 1]\n",
    "            metrics = get_metrics(test_labels[data_name], score)\n",
    "            print(predictor, metrics)\n",
    "            print()\n",
    "            result_records.append({\"Model Size\": model_size, \n",
    "                                   \"Constraint\": constraint_names[data_name],\n",
    "                                   \"Data\": rf\"{data_pretty[data_name]}\", \n",
    "                                   \"BaseRate\": y_test.mean(), \n",
    "                                   \"Predictor\": predictor, \n",
    "                                   **metrics})\n",
    "df_results = pd.DataFrame(result_records)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d3ac3e4-b4cf-4477-b5aa-151f2f2958c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_context(\"paper\")\n",
    "sns.set_palette(\"deep\")\n",
    "df_results_cp = df_results.copy()\n",
    "df_results_cp[\"AUROC\"] = df_results_cp[r\"AUROC$\\textcolor{Green}{\\mathbf{(\\Uparrow)}}$\"]\n",
    "for metric in [\"AUROC\"]:\n",
    "\n",
    "    df_results_cp[\"DS+C\"] = df_results_cp.apply(lambda x: x[\"Data\"]+\" | \"+x[\"Constraint\"], axis=1)\n",
    "    fig, axs = plt.subplots(1, 3, figsize=(10, 2))\n",
    "    for i, ms in enumerate(df_results_cp[\"Model Size\"].unique()):\n",
    "        df_results_model = df_results_cp[df_results_cp[\"Model Size\"] == ms]\n",
    "        model = f\"Llama-2 {ms.upper()}\"\n",
    "        ax = axs[i]\n",
    "        axs[i].set_title(model)\n",
    "        sns.scatterplot(data=df_results_model, x=metric,y=\"DS+C\",  hue=\"Predictor\", s=52, ax=ax)\n",
    "        #sns.barplot(data=df_results_model, x=metric,y=\"DS+C\",  hue=\"Predictor\", ax=ax)\n",
    "\n",
    "        if i != 0:\n",
    "            ax.set_yticks([])\n",
    "    \n",
    "        if i != 1:\n",
    "            ax.get_legend().remove()\n",
    "        \n",
    "        if i == 1:\n",
    "            ax.legend(loc=\"upper left\")\n",
    "        \n",
    "        ax.set_ylabel(\"\")\n",
    "    \n",
    "    fig.tight_layout()\n",
    "    fig.savefig(f\"./figures/figure5_{metric}.pdf\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
