{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c670b192-bebf-4382-ad30-af7de8d4848f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44e0d3d8-4623-4168-8f4d-36b8acb6c64c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_basics = pd.read_csv(\"/home/t-merty/mounts/sandbox-mert/data/ImdbTitleBasics.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f043ca2-fd1a-47a8-b9e7-390b996fd19c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_crew = pd.read_csv(\"/home/t-merty/mounts/sandbox-mert/data/ImdbTitlePrincipals.csv\")\n",
    "df_crew = df_crew[df_crew.category.isin([\"actor\", \"actress\"])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07ff5b97-cc1f-4b6b-94fa-c0736987a3c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_names = pd.read_csv(\"/home/t-merty/mounts/sandbox-mert/data/ImdbName.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dea327cc-fcb6-4fc7-8eb4-e4d5819e0de3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_basics.columns, df_crew.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3eed6fa-ac3c-4c31-bc9d-9177143a163c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_crew = pd.merge(df_crew, df_basics[['tconst', 'primaryTitle']], on='tconst', how='left')\n",
    "df_crew = pd.merge(df_crew, df_names[['nconst', 'primaryName']], on='nconst', how='left')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f784094-5a4c-4df5-858d-d1f9dc37e2d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_basics[df_basics.titleType == \"movie\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dacccced-e044-4a73-b8b7-0ff95f28e875",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c28d4f69-5c8f-49db-b1ab-b557456b8fd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "nobel_json = json.load(open(\"/home/t-merty/mounts/sandbox-mert/data/nobel_winners.json\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d30d2f82-b3a4-4568-a4f3-0f0d4284e320",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_nobel = pd.DataFrame(nobel_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f791781-72c5-4659-90e7-3023844acb13",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_nobel.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ee1b3b1-0094-4fef-b9a1-a3564606e566",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_nobel[df_nobel.borncity == \"Todmorden\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36a894db-3b0e-43ef-8caa-2e0d99bc0ce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "nobel_items = []\n",
    "for city in df_nobel.borncity.unique():\n",
    "    prompt = f\"User: Is there a person who was born in the city of {city} and who is a Nobel Prize Winner\\nAssistant: Yes, the person's name is\"\n",
    "    nobel_items.append({\"constraints\": [\" who is a Nobel Prize Winner\", f\" who was born in the city of {city}\"],\n",
    "                        \"prompt\": prompt, \n",
    "                        \"popularity\": -1\n",
    "                       })\n",
    "    print(prompt)\n",
    "\n",
    "with open(\"/home/t-merty/mounts/sandbox-mert/data/nobel_data.pkl\", \"wb\") as f:\n",
    "    pickle.dump(nobel_items, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deffe295-9805-445b-b8bf-b493296c6579",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/home/t-merty/mounts/sandbox-mert/data/nobel_data.pkl\", \"rb\") as f:\n",
    "    nobel_items = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2593863-1816-4ef4-a3cd-ffcbe78edf14",
   "metadata": {},
   "outputs": [],
   "source": [
    "nobel_items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4cbda47-a502-4a2f-860b-743a03e664f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "query = \"\"\"\n",
    "SELECT DISTINCT ?senator ?senatorLabel ?stateLabel ?schoolLabel ?popularity WHERE {\n",
    "  ?senator p:P39 ?statement.\n",
    "  ?statement ps:P39 wd:Q4416090.\n",
    "  OPTIONAL { ?statement pq:P768 ?state. }  # <- state represented\n",
    "  ?senator wdt:P31 wd:Q5;  # <- instance of human\n",
    "           p:P69 ?educationStatement;  # <- educational institution\n",
    "           wikibase:sitelinks ?popularity.   # <- popularity (site links)\n",
    "  ?educationStatement ps:P69 ?school.  # <- school\n",
    "  SERVICE wikibase:label { bd:serviceParam wikibase:language \"[AUTO_LANGUAGE],en\". }\n",
    "}\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "url = \"https://query.wikidata.org/sparql\"\n",
    "params = {\"query\": query, \"format\": \"json\"}\n",
    "\n",
    "response = requests.get(url, params=params)\n",
    "data = response.json()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca4b5100-b8d9-447a-8559-c28edcaffb84",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_senators = []\n",
    "for i, item in enumerate(data['results']['bindings']):\n",
    "    all_senators.append(item['senatorLabel']['value'])\n",
    "all_senators = list(set(all_senators))\n",
    "all_senators = [f\"{i+1}. {s}\" for i, s in enumerate(all_senators)]\n",
    "all_senators_txt = \"\\n\".join(all_senators)\n",
    "records = []\n",
    "for item in data['results']['bindings']:\n",
    "    if \"stateLabel\" not in item:\n",
    "        continue\n",
    "    records.append({\"Name\": item['senatorLabel']['value'], \n",
    "                   \"School\": item['schoolLabel']['value'], \n",
    "                   \"State\": item['stateLabel']['value'].split(\" Class\")[0], \n",
    "                    \"Popularity\": item['popularity']['value'], \n",
    "                   })\n",
    "df_all_senators = pd.DataFrame(records)\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "455fce43-6999-48ca-a8d2-67d6504c9796",
   "metadata": {},
   "outputs": [],
   "source": [
    "hall_senators = pd.read_csv(\"./rare_knowledge/data/hallucination_senator.csv\")\n",
    "hall_senators[\"state\"] = hall_senators[\"prompt\"].apply(lambda x: x.split(\"state of \")[1].split(\" \")[0])\n",
    "hall_senators[\"school\"] = hall_senators[\"prompt\"].apply(lambda x: x.split(\"alma mater was \")[-1].split(\"?\")[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53eb3401-5cb1-41ff-9f46-ea75396aa6a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "records = []\n",
    "for item in data['results']['bindings']:\n",
    "    if \"stateLabel\" not in item:\n",
    "        continue\n",
    "    if (\"university\" not in item['schoolLabel']['value'].lower()) and (\"college\" not in item['schoolLabel']['value'].lower()):\n",
    "        continue\n",
    "    if not (len(item[\"schoolLabel\"][\"value\"].split(\" \")) <= 2):\n",
    "        continue\n",
    "    records.append({\"Name\": item['senatorLabel']['value'], \n",
    "                   \"School\": item['schoolLabel']['value'], \n",
    "                   \"State\": item['stateLabel']['value'].split(\" Class\")[0], \n",
    "                    \"Popularity\": item['popularity']['value'], \n",
    "                   })\n",
    "df_questions = pd.DataFrame(records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f56a7126-cfd7-4163-b775-dc2ec57f3452",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "num_to_sample = 5\n",
    "items = []\n",
    "df_questions.sort_values(\"Popularity\", ascending=False, inplace=True)\n",
    "for state in df_questions.State.unique():\n",
    "    prompt_template = \"\"\"Fill in the blanks.\\nOccupation: Senator\\nState: {}\\nAlma Mater: {}\\nName:\"\"\"\n",
    "    hall_school = hall_senators[hall_senators.state == state].school.unique()    \n",
    "    item_per_state = np.min([num_to_sample, len(hall_school), df_questions[df_questions.State == state].shape[0]])\n",
    "    if len(hall_school) == 0:\n",
    "        continue\n",
    "    hall_schools = np.random.choice(hall_school, item_per_state, replace=False)\n",
    "    for hall_school in hall_schools:\n",
    "        hall_prompt = prompt_template.format(state, hall_school)\n",
    "        items.append({\"constraints\": [\"State: {}\".format(state), \"Alma Mater: {}\".format(hall_school)], \"prompt\": hall_prompt, \"exists\": 0,\n",
    "                     \"popularity\": -1})\n",
    "    for i in range(item_per_state):\n",
    "        exists_school = df_questions[df_questions.State == state].iloc[i][\"School\"]\n",
    "        exists_prompt = prompt_template.format(state, exists_school)\n",
    "        items.append({\"constraints\": [\"State: {}\".format(state), \"Alma Mater: {}\".format(exists_school)], \"prompt\": exists_prompt, \"exists\": 1,\n",
    "                     \"popularity\": -1})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78d9436c-5201-4c2f-97f5-9ba6e9f4cf91",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/home/t-merty/mounts/sandbox-mert/data/senator_multiconstraint_data.pkl\", \"wb\") as f:\n",
    "    pickle.dump(items, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58142c5b-04b7-4590-a2eb-f997eb278aaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/home/t-merty/mounts/sandbox-mert/data/senator_multiconstraint_data.pkl\", \"rb\") as f:\n",
    "    pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "828081a3-d6e5-40d4-97cd-ba92848b2a03",
   "metadata": {},
   "source": [
    "## Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07fe257d-e54e-4b1c-97c7-4b2447ef7062",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from nltk.corpus import words\n",
    "import random\n",
    "import string\n",
    "\n",
    "all_words = words.words()\n",
    "def satisfies_conditions(word, conditions):\n",
    "    for pos, letter in conditions:\n",
    "        try:\n",
    "            if word[pos] != letter: return False\n",
    "        except IndexError:\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "def count_satisfying_words(conditions):\n",
    "    return sum(satisfies_conditions(w, conditions) for w in all_words)\n",
    "\n",
    "\n",
    "def sample_words():\n",
    "    items = []\n",
    "    constraint_set = list()\n",
    "    all_letters = list(string.ascii_lowercase)\n",
    "    \n",
    "    for first in tqdm(all_letters):\n",
    "        for last in all_letters:\n",
    "            constraints = [f\" starts with the letter {first}\", f\" ends with the letter {last}\"]\n",
    "            letters = [first, last]\n",
    "        \n",
    "            popularity = count_satisfying_words([[0, first], [-1, last]])\n",
    "                \n",
    "            full_prompt = f\"User: Is there a word that{constraints[0]} and{constraints[1]}\\nAssistant: Yes, one such word is\"\n",
    "            items.append({\"prompt\": full_prompt, \"constraints\": constraints,\n",
    "                          \"conditions\": [[0, first], [-1, last]],\n",
    "                         \"popularity\": popularity})\n",
    "\n",
    "            constraints = [f\" ends with the letter {last}\", f\" starts with the letter {first}\"]\n",
    "            letters = [first, last]\n",
    "                        \n",
    "            full_prompt = f\"User: Is there a word that{constraints[0]} and{constraints[1]}\\nAssistant: Yes, one such word is\"\n",
    "            items.append({\"prompt\": full_prompt, \"constraints\": constraints,\n",
    "                          \"conditions\": [[0, first], [-1, last]],\n",
    "                         \"popularity\": popularity})\n",
    "            \n",
    "    return items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8981fc7-c145-4935-a9f1-325b2de3560a",
   "metadata": {},
   "outputs": [],
   "source": [
    "items = sample_words()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19495048-ea09-413a-8520-3ad93e68bda7",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/home/t-merty/mounts/sandbox-mert/data/word_startend_multiconstraint_data.pkl\", \"wb\") as f:\n",
    "    pickle.dump(items, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fcbac61-5915-4968-a0d0-84d9ab224afb",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/home/t-merty/mounts/sandbox-mert/data/word_startend_multiconstraint_data.pkl\", \"rb\") as f:\n",
    "    items = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd54ac77-c506-4567-a2b1-eecdfe9f4112",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(items))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
